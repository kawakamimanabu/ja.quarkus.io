msgid ""
msgstr ""
"Language: ja_JP\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: jekyll-l10n\n"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Crafting a Local RAG application with Quarkus"
msgstr "Quarkusを使用したローカルRAGアプリケーションの作成"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "How to build a RAG chatbot using the Granite LLM."
msgstr "Granite LLMを使ってRAGチャットボットを構築する方法。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"This blog post demonstrate how to build an AI-infused chatbot application using Quarkus, LangChain4j, https://infinispan.org/[Infinispan], and the https://github.com/ibm-granite/granite-code-models[Granite LLM].\n"
"In this post, we will create an **entirely** local solution, eliminating the need for any cloud services, including the LLM."
msgstr "このブログポストでは、Quarkus、LangChain4j、 link:https://infinispan.org/[Infinispan] 、 link:https://github.com/ibm-granite/granite-code-models[Granite LLMを] 使用して、AIを組み込んだチャットボットアプリケーションを構築する方法を紹介します。この投稿では、LLMを含むクラウドサービスを必要としない、 *完全に* ローカルなソリューションを作成します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Our chatbot leverages the Granite LLM, a language model that generates contextually relevant text based on user prompts.\n"
"To run Granite locally, we'll be utilizing https://instructlab.ai/[InstructLab], though https://github.com/containers/podman-desktop-extension-ai-lab[Podman AI Lab] is another viable option."
msgstr "私たちのチャットボットは、Granite LLMを活用しています。Granite LLMは、ユーザーのプロンプトに基づいてコンテキストに関連したテキストを生成する言語モデルです。Graniteをローカルで動かすには、 link:https://instructlab.ai/[InstructLabを] 利用しますが、 link:https://github.com/containers/podman-desktop-extension-ai-lab[Podman AI Labも] 有効な選択肢です。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The core of our application is based on the RAG (Retrieval-Augmented Generation) pattern.\n"
"This approach enhances the chatbot's responses by retrieving pertinent information from a vector database — in this case, Infinispan — before generating a response."
msgstr "私たちのアプリケーションの中核は、RAG（Retrieval-Augmented Generation）パターンに基づいています。このアプローチは、応答を生成する前にベクトルデータベース（この場合はInfinispan）から適切な情報を取得することで、チャットボットの応答を強化します。"

#: _posts/2024-06-14-granite-rag.adoc
msgid "Architecture"
msgstr "アーキテクチャ"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Our chatbot application is composed of four main components:"
msgstr "私たちのチャットボットアプリケーションは、4つの主要コンポーネントで構成されています："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"**Web Socket Endpoint**: This component serves as the communication bridge between the chatbot's backend and the frontend interface.\n"
"This component uses the new Quarkus WebSocket-Next extension to handle WebSocket connections efficiently.\n"
"It relies on the AI Service to interact with the LLM."
msgstr "*ウェブソケットエンドポイント* ：このコンポーネントは、チャットボットのバックエンドとフロントエンドインターフェース間の通信ブリッジの役割を果たします。このコンポーネントは、新しいQuarkus WebSocket-Nextエクステンションを使用して、WebSocket接続を効率的に処理します。LLMとのやり取りは、AIサービスに依存します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**Ingestor**: The ingestor is responsible for populating the database with relevant data. It processes a set of local documents, split them into text segments, compute their vector representation and store them into Infinispan."
msgstr "*インジェスター* ：インジェスターはデータベースに関連データを投入する役割を果たします。ローカルドキュメントのセットを処理し、テキストセグメントに分割し、ベクトル表現を計算し、Infinispanに格納します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**Retriever**: The retriever allows finding relevant text segments into Infinispan. When a user inputs a query, the retriever searches the vector database to find the most relevant pieces of information."
msgstr "*レトリバー* ：レトリーバーはInfinispanに関連するテキストセグメントを検索します。ユーザがクエリを入力すると、レトリーバはベクトルデータベースを検索して最も関連性の高い情報を見つけます。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**AI Service**: This is the core component of the chatbot, combining the capabilities of the retriever and the Granite LLM. The AI service takes the relevant information fetched by the retriever and uses the Granite LLM to generate the appropriate responses."
msgstr "*AIサービス* ：これはチャットボットの中核となるコンポーネントで、レトリーバーとグラニットLLMの機能を組み合わせています。AIサービスは、レトリーバーによって取得された関連情報を取り込み、Granite LLMを使用して適切な応答を生成します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The following picture illustrates the high-level architecture:"
msgstr "次の図は、高レベルのアーキテクチャを表しています："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "A simple explanation of the RAG pattern"
msgstr "RAGパターンの簡単な説明"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The RAG (Retrieval-Augmented Generation) pattern is one of the most popular AI patterns, combining a retrieval mechanism with a generation mechanism to provide more accurate and relevant responses."
msgstr "RAG (Retrieval-Augmented Generation) パターンは、最も一般的な AI パターンの 1 つで、検索メカニズムと生成メカニズムを組み合わせることで、より正確で適切な応答を提供します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The RAG pattern operates in two main steps:"
msgstr "RAGパターンは主に2つのステップで動作します："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**Ingestion**: The application ingests a set of documents, processes them, and stores them in a vector database."
msgstr "*取り込み* ：アプリケーションは一連の文書を取り込み、処理し、ベクターデータベースに保存します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "**Retrieval**: When a user inputs a query, the application retrieves the most relevant information from the vector database."
msgstr "*検索* ：ユーザーがクエリを入力すると、アプリケーションはベクトルデータベースから最も関連性の高い情報を検索します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The following image gives a high-level overview of the _traditional_ RAG pattern:"
msgstr "次の画像は、 _従来の_ RAGパターンのハイレベルな概要を示しています："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "There are more advanced version of the RAG pattern, but let's stick to the basics for this application."
msgstr "RAGパターンにはもっと高度なものもありますが、今回は基本に忠実に説明しましょう。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Ingestion"
msgstr "摂取"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Let's first look at the ingestion step.\n"
"The ingestion process involves reading a set of documents, splitting them into text segments, computing their vector representations, and storing them in Infinispan."
msgstr "まず、取り込みのステップを見てみましょう。取り込みプロセスでは、一連のドキュメントを読み込み、テキストセグメントに分割し、ベクトル表現を計算し、Infinispanに格納します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The secret of an effective RAG implementation lies in how the text segments are computed.\n"
"In our application, we will follow a straightforward approach, but more advanced techniques are available and often required.\n"
"Depending on the document, you can use a variety of techniques to split the text into segments, such as paragraph splitting, sentence splitting, or more advanced techniques like the `recursive` splitter.\n"
"Also, if the document has a specific structure, you can use this structure to split the text into segments (like sections, chapters, etc.)."
msgstr "効果的な RAG 実装の秘訣は、テキストセグメントの計算方法にあります。このアプリケーションでは単純なアプローチを使用しますが、 より高度なテクニックも使用可能であり、多くの場合それを必要とします。段落分割や文分割、あるいは `recursive` スプリッターのような高度なテクニックなど、 ドキュメントに応じてさまざまなテクニックを使ってテキストを分割することができます。また、文書に特定の構造がある場合は、その構造を使ってテキストをセグメント (セクションや章など) に分割することもできます。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The embedding model is responsible for converting text into a vector representation.\n"
"For simplicity, we use an in-process embedding model (`BGE-small`).\n"
"Other options, like the Universal Angle Embedding, are available, but we'll stick with BGE-small for simplicity."
msgstr "埋め込みモデルは、テキストをベクトル表現に変換する役割を果たします。簡単のため、ここではプロセス内埋め込みモデル ( `BGE-small` ) を使います。Universal Angle Embeddingのような他のオプションも利用可能ですが、ここでは単純化のためにBGE-smallにこだわります。"

#: _posts/2024-06-14-granite-rag.adoc
msgid "Retrieval"
msgstr "取得"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The second step is the retrieval process.\n"
"When a user inputs a query, the application searches the vector database to find the most relevant text segments."
msgstr "第二段階は検索プロセスです。ユーザーがクエリを入力すると、アプリケーションはベクトルデータベースを検索し、最も関連性の高いテキストセグメントを見つけます。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"To achieve this, the application computes the vector representation of the user query using the **same** embedding model and compares it with the vector representations of the stored text segments in Infinispan.\n"
"It selects the most relevant text segments based on the similarity between the query vector and the text segment vectors."
msgstr "これを実現するために、アプリケーションは *同じ* 埋め込みモデルを使用してユーザークエリのベクトル表現を計算し、Infinispan に保存されているテキストセグメントのベクトル表現と比較します。クエリーベクトルとテキストセグメントベクトルの類似度に基づいて、最も関連性の高いテキストセグメントを選択します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Then, it augments the user query with the retrieved text segments and sends it to the LLM.\n"
"Note that until this step, the LLM is not used."
msgstr "次に、検索されたテキストセグメントでユーザークエリを補強し、LLM に送信します。このステップまで、LLM は使用されません。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Implementing the chatbot"
msgstr "チャットボットの実装"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Enough theory—let's dive into the implementation.\n"
"You can find the final version on https://github.com/cescoffier/quarkus-granite-rag-demo[GitHub]."
msgstr "理論はもう十分です。最終版は link:https://github.com/cescoffier/quarkus-granite-rag-demo[GitHubに] あります。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Used extensions and dependencies"
msgstr "使用されるエクステンションと依存関係"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "To implement our chatbot, we rely on the following Quarkus extensions:"
msgstr "チャットボットを実装するには、次のQuarkusのエクステンションを使用します："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "`quarkus-langchain4j-openai`: Integrates LLM providers using the OpenAI API, suitable for both InstructLab and Podman AI Lab."
msgstr "`quarkus-langchain4j-openai` :InstructLabとPodman AI Labの両方に適しています。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "`quarkus-websockets-next`: Provides support for WebSocket communication."
msgstr "`quarkus-websockets-next` :WebSocket 通信のサポートを提供します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "`quarkus-langchain4j-infinispan`: Integrates Infinispan with LangChain4j, allowing us to store and retrieve vector representations of text segments."
msgstr "`quarkus-langchain4j-infinispan` :InfinispanとLangChain4jを統合し、テキストセグメントのベクトル表現を保存・取得できるようにします。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "`quarkus-web-bundler`: Bundles frontend resources with the Quarkus application."
msgstr "`quarkus-web-bundler` :Quarkusアプリケーションにフロントエンドリソースをバンドルします。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "We also need a specific dependency to use the BGE-small embedding model:"
msgstr "また、BGE-smallエンベッディングモデルを使用するには、特定の依存関係が必要です："

#: _posts/2024-06-14-granite-rag.adoc
msgid "Configuration"
msgstr "設定"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "We need a bit of configuration to ensure our application uses Granite and sets up the Infinispan database correctly:"
msgstr "アプリケーションがGraniteを使用し、Infinispanデータベースを正しくセットアップするために、少し設定が必要です："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Configure the dimension of the vectors stored in Infinispan, which depends on the embedding model (BGE-small in our case)."
msgstr "Infinispanに格納されるベクトルの次元を設定します。これはエンベッディングモデル（ここではBGE-small）に依存します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Configure the OpenAI service to use InstructLab.\n"
"You can replace the base URL with the one for Podman AI Lab if you prefer.\n"
"Indeed, InstructLab and Podman AI Lab expose an OpenAI-compatible API."
msgstr "InstructLabを使うようにOpenAIサービスを設定します。ベース URL を Podman AI Lab 用のものに置き換えることもできます。実際、InstructLabとPodman AI LabはOpenAI互換のAPIを公開しています。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Set the default embedding model to BGE-small."
msgstr "デフォルトの埋め込みモデルをBGE-smallに設定します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The Ingestor"
msgstr "インゲスター"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"With the configuration in place, let's implement the https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/Ingestion.java[ingestion part (Ingestion.java)].\n"
"The ingestor reads documents from the `documents` directory, splits them into text segments, computes their vector representations, and stores them in Infinispan:"
msgstr "設定が整ったところで、 link:https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/Ingestion.java[インジェスト部分(Ingestion.java] )を実装してみましょう。インジェスターは、 `documents` ディレクトリからドキュメントを読み込み、テキストセグメントに分割し、ベクトル表現を計算し、Infinispan に格納します："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@Startup` annotation ensures that the ingestion process starts when the application launches."
msgstr "`@Startup` アノテーションは、アプリケーションの起動時に取り込みプロセスが開始されるようにします。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `Ingestion` class uses an (automatically injected) `EmbeddingStore<TextSegment>` (Infinispan) and an `EmbeddingModel` (BGE-small)."
msgstr "`Ingestion` クラスは、(自動的に注入される) `EmbeddingStore<TextSegment>` (Infinispan) と `EmbeddingModel` (BGE-small) を使用します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"We use a simple document splitter (`recursive(1024, 0)`) to split the documents into text segments.\n"
"More advanced techniques may be used to improve the accuracy of the RAG model."
msgstr "文書をテキストセグメントに分割するには、単純な文書分割ツール ( `recursive(1024, 0)` ) を使用します。RAGモデルの精度を向上させるために、より高度な技術を使用することもできます。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The retriever"
msgstr "レトリーバー"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Next, let's implement the https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/Retriever.java[retriever (Retriever.java)].\n"
"The retriever finds the most relevant text segments in Infinispan based on the user query:"
msgstr "次に、レトリバーを実装しましょう link:https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/Retriever.java[(Retriever.java)] 。リトリーバは、ユーザークエリに基づいて Infinispan 内の最も関連性の高いテキストセグメントを検索します："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"To implement a retriever, expose a bean that implements the `Supplier<RetrievalAugmentor>` interface.\n"
"The `Retriever` class uses `EmbeddingStore<TextSegment>` (Infinispan) and `EmbeddingModel` (BGE-small) to build the retriever."
msgstr "リトリーバを実装するには、 `Supplier<RetrievalAugmentor>` インタフェースを実装するビーンを公開します。 `Retriever` クラスは、 `EmbeddingStore<TextSegment>` (Infinispan) と `EmbeddingModel` (BGE-small) を使用してリトリーバを構築します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"The `maxResults` method in the EmbeddingStoreContentRetriever builder specifies the number of text segments to retrieve.\n"
"Since our segments are large, we retrieve only two segments."
msgstr "EmbeddingStoreContentRetriever ビルダの `maxResults` メソッドでは、取得するテキストセグメントの数を指定します。今回のセグメントは大きいので、2 つのセグメントだけを取得します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The AI Service"
msgstr "AIサービス"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/ChatBot.java[AI Service (ChatBot.java)] is the core component of our chatbot, combining the capabilities of the retriever and the Granite LLM to generate appropriate responses."
msgstr "link:https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/ChatBot.java[AIサービス（ChatBot.java] ）は私たちのチャットボットの中核となるコンポーネントで、レトリーバーとグラナイトLLMの機能を組み合わせて適切な応答を生成します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "With Quarkus, implementing an AI service is straightforward:"
msgstr "Quarkusでは、AIサービスを簡単に導入できます："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@RegisterAiService` annotation specifies the retrieval augmentor to use, which in our case is the `Retriever` bean defined earlier."
msgstr "`@RegisterAiService` アノテーションは、使用する検索オーグメンターを指定します。この場合は、先に定義した `Retriever` ビーンです。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@SystemMessage` annotation provides the main instructions for the AI model."
msgstr "`@SystemMessage` 注釈は、AIモデルの主な指示を提供します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@SessionScoped` annotation ensures that the AI service is stateful, maintaining context between user interactions for more engaging conversations."
msgstr "`@SessionScoped` 注釈は、AIサービスがステートフルであることを保証し、より魅力的な会話のためのユーザーインタラクション間のコンテキストを維持します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `ChatBot` interface defines a single method, `chat`, which takes a user question as input and returns the chatbot's response."
msgstr "`ChatBot` インターフェースは、ユーザーの質問を入力として受け取り、チャットボットの応答を返す単一のメソッド、 `chat` を定義しています。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The WebSocket endpoint"
msgstr "WebSocket エンドポイント"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The final piece is the https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/ChatWebSocket.java[WebSocket endpoint (ChatWebSocket.java)], which serves as the communication bridge between the chatbot's backend and the frontend interface:"
msgstr "最後のピースは link:https://github.com/cescoffier/quarkus-granite-rag-demo/blob/main/src/main/java/me/escoffier/granite/rag/ChatWebSocket.java[WebSocketエンドポイント(ChatWebSocket.java)] で、チャットボットのバックエンドとフロントエンドのインターフェイス間の通信ブリッジとして機能します："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@WebSocket` annotation specifies the WebSocket path."
msgstr "`@WebSocket` アノテーションは、WebSocket のパスを指定します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@OnOpen` method sends a welcome message when a user connects to the _WebSocket_."
msgstr "`@OnOpen` メソッドは、ユーザーが _WebSocket_ に接続したときに歓迎メッセージを送信します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "The `@OnTextMessage` method processes the user's messages and returns the chatbot's responses, using the injected AI service."
msgstr "`@OnTextMessage` メソッドは、注入されたAIサービスを使用して、ユーザーのメッセージを処理し、チャットボットの応答を返します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "That's it! Our chatbot is now ready to chat with users, providing contextually relevant responses based on the RAG pattern."
msgstr "これで完了です！チャットボットは、RAGパターンに基づいてコンテキストに関連した応答を提供し、ユーザーとチャットする準備が整いました。"

#: _posts/2024-06-14-granite-rag.adoc
msgid "Running the application"
msgstr "アプリケーションの実行"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Let's run the application and see our chatbot in action.\n"
"First, clone the https://github.com/cescoffier/quarkus-granite-rag-demo/tree/main[repository] and run the following command:"
msgstr "アプリケーションを実行して、チャットボットの動きを見てみましょう。まず、 link:https://github.com/cescoffier/quarkus-granite-rag-demo/tree/main[リポジトリを] クローンして以下のコマンドを実行します："

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"This command starts the Quarkus application in development mode.\n"
"Ensure you have InstructLab or Podman AI Lab running to use the Granite LLM.\n"
"You will also need Docker or Podman to automatically start Infinispan."
msgstr "このコマンドは、Quarkusアプリケーションを開発モードで起動します。Granite LLMを使用するには、InstructLabまたはPodman AI Labが起動していることを確認してください。また、Infinispanを自動的に起動するには、DockerまたはPodmanが必要です。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid "Podman AI Lab or InstructLab?"
msgstr "Podman AI LabかInstructLabか？"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"You can use either Podman AI Lab or InstructLab to run the Granite LLM locally.\n"
"Depending on the OS, Podman may not have GPU support. Thus, response time can be high.\n"
"In this case, InstructLab is the preferred option for better response times.\n"
"Typically, on a Mac, you would use InstructLab, while on Linux, Podman AI Lab shows great performances."
msgstr "ローカルでGranite LLMを実行するには、Podman AI LabまたはInstructLabのいずれかを使用できます。OSによっては、PodmanがGPUをサポートしていない場合があります。そのため、応答時間が長くなることがあります。この場合、InstructLabの方が応答時間が短縮されます。通常、MacではInstructLabを使用し、LinuxではPodman AI Labが素晴らしいパフォーマンスを示します。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"Once the application is up and running, open your browser and navigate to http://localhost:8080.\n"
"You should see the chatbot interface, where you can start chatting with Mona:"
msgstr "アプリケーションが起動したら、ブラウザを開き、 http://localhost:8080 。チャットボットのインターフェイスが表示され、Monaとチャットを始めることができます："

#: _posts/2024-06-14-granite-rag.adoc
msgid "Summary"
msgstr "概要"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"That's it!\n"
"With just a few lines of code, we have implemented a chatbot using the RAG pattern, combining the capabilities of the Granite LLM, Infinispan, and Quarkus.\n"
"This application runs entirely locally, eliminating the need for any cloud services and addressing privacy concerns."
msgstr "以上です！わずか数行のコードで、Granite LLM、Infinispan、Quarkusの機能を組み合わせたRAGパターンを使用してチャットボットを実装しました。このアプリケーションは完全にローカルで実行されるため、クラウドサービスを利用する必要がなく、プライバシーに関する懸念にも対応できます。"

#: _posts/2024-06-14-granite-rag.adoc
#, fuzzy
msgid ""
"This is just an example of what you can achieve with the Quarkus LangChain4j extension.\n"
"You can easily extend this application by adding more advanced features, such as sophisticated document splitters, embedding models, or retrieval mechanisms.\n"
"Quarkus LangChain4J also provides support for https://docs.langchain4j.dev/tutorials/rag/#advanced-rag[_advanced_ RAG], many other LLM and embedding models and vector stores.\n"
"Find out more on https://docs.quarkiverse.io/quarkus-langchain4j/dev/index.html[Quarkus LangChain4J]."
msgstr "これは、Quarkus LangChain4jエクステンションで実現できることのほんの一例です。洗練されたドキュメント分割、埋め込みモデル、検索メカニズムなど、より高度な機能を追加することで、このアプリケーションを簡単に拡張できます。Quarkus LangChain4Jでは、 link:https://docs.langchain4j.dev/tutorials/rag/#advanced-rag[高度なRAG] 、その他多くのLLM、埋め込みモデル、ベクトルストアもサポートしています。詳細は link:https://docs.quarkiverse.io/quarkus-langchain4j/dev/index.html[Quarkus LangChain4J] をご覧ください。"
