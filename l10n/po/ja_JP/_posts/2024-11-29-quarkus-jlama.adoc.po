msgid ""
msgstr ""
"Language: ja_JP\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: jekyll-l10n\n"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Creating pure Java LLM infused application with Quarkus, Langchain4j and Jlama"
msgstr "Quarkus、Langchain4j、Jlamaを使用した純粋なJava LLMアプリケーションの作成"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Creating pure Java LLM infused application with Quarkus, LangChain4j and Jlama"
msgstr "Quarkus、LangChain4j、Jlamaによる純粋なJava LLM注入アプリケーションの作成"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Currently the vast majority of LLM-based applications rely on external services provided by specialized companies. These services typically offer the access to huge, general purpose models, implying energy consumption and then costs that are proportional to the size of these models."
msgstr "現在、LLMベースのアプリケーションの大部分は、専門企業が提供する外部サービスに依存しています。これらのサービスは通常、巨大な汎用モデルへのアクセスを提供するため、エネルギー消費量やコストはこれらのモデルのサイズに比例します。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Even worse, this usage pattern also comes with both privacy and security concerns, since it is virtually impossible to be sure how those service providers will eventually re-use the prompts of their customers, which in some cases could also contain sensitive information."
msgstr "さらに悪いことに、この使用パターンにはプライバシーとセキュリティの両方の懸念が伴います。なぜなら、これらのサービスプロバイダが顧客のプロンプトを最終的にどのように再利用するかを確認することは事実上不可能であり、場合によっては機密情報も含まれている可能性があるからです。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "For these reasons many companies are exploring the option of training or fine-tuning smaller models that do not claim to be usable in a general context, but that are tailored towards specific business needs and subsequently running (serving in LLM terms) these models on premise or on private clouds."
msgstr "このような理由から、多くの企業は、一般的なコンテキストで使用可能とは主張しないが、特定のビジネスニーズに合わせた小規模なモデルをトレーニングまたは微調整し、その後、これらのモデルをオンプレミスまたはプライベートクラウド上で実行（LLM用語でサービング）するというオプションを模索しています。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The features provided by these specialized models need to be integrated into the existing software infrastructure, that in the enterprise world are very often written in Java. This could be accomplished following a traditional client-server architecture, for instance serving the model through an external server like https://ollama.com/[Ollama] and querying it through REST calls. While this should not present any particular problem for Java developers, they could work more efficiently, if they could consume the model directly in Java and without any need to install additional tools. Finally the possibility of embedding the LLM interaction directly in the same Java process running the application will make it easier to move from local dev to deployment, relieving IT from the burden of managing an external server, thus bypassing the need for a more mature platform engineering strategy. This is where Jlama comes into play."
msgstr "これらの特殊なモデルによって提供される機能は、既存のソフトウェア・インフラストラクチャに統合される必要があります。これは、例えば link:https://ollama.com/[Ollamaの] ような外部サーバーを通してモデルを提供し、REST呼び出しを通してクエリを実行するといった、伝統的なクライアント・サーバー・アーキテクチャに従って達成することができます。Java開発者にとっては特に問題はないはずですが、Javaで直接モデルを利用することができれば、追加のツールをインストールする必要なく、より効率的に作業することができます。最後に、アプリケーションを実行している同じJavaプロセスにLLMインタラクションを直接組み込むことができれば、ローカル開発からデプロイメントへの移行が容易になり、IT部門は外部サーバーの管理負担から解放されます。ここでJlamaの出番です。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "How and why executing LLM inference in pure Java with Jlama"
msgstr "Jlamaによる純粋なJavaでのLLM推論の実行方法と理由"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "https://github.com/tjake/Jlama[Jlama] is a library allowing to execute LLM inference in pure Java. It supports many LLM model families like Llama, Mistral, Qwen2 and Granite. It also implements out-of-the-box many useful LLM related features like functions calling, models quantization, mixture of experts and even distributed inference."
msgstr "link:https://github.com/tjake/Jlama[Jlamaは] 、純粋なJavaでLLM推論を実行できるライブラリです。Llama、Mistral、Qwen2、Graniteなど多くのLLMモデルファミリーをサポートしています。また、関数呼び出し、モデルの量子化、エキスパートの混合、分散推論など、LLMに関連する多くの便利な機能をすぐに実装できます。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Jlama is well integrated with Quarkus through the https://quarkus.io/extensions/io.quarkiverse.langchain4j/quarkus-langchain4j-jlama/[dedicated lanchain4j based extension]. Note that for performance reasons Jlama uses the https://openjdk.org/jeps/469[Vector API] which is still in preview in Java 23, and very likely will be released as a supported feature in Java 25."
msgstr "Jlamaは、 link:https://quarkus.io/extensions/io.quarkiverse.langchain4j/quarkus-langchain4j-jlama/[専用のlanchain4jベースの拡張機能を通して] Quarkusとうまく統合されています。パフォーマンス上の理由から、JlamaはJava 23ではまだプレビュー中であり、Java 25ではサポートされる機能としてリリースされる可能性が非常に高い link:https://openjdk.org/jeps/469[Vector APIを] 使用していることに注意してください。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "In essence Jlama makes it possible to serve an LLM in Java, directly embedded in the same JVM running your Java application, but why could this be useful? Actually this is desirable in many use cases and presents a number of relevant advantages like the following:"
msgstr "要するにJlamaは、Javaアプリケーションを実行している同じJVMに直接組み込んで、JavaでLLMを提供することを可能にします。実際、これは多くのユースケースで望ましく、以下のような多くの関連する利点があります："

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Similar lifecycle between model and app*: There can be use cases where the model and the application using it have the same lifecycle, so that the development of a new feature in the application also requires a change in the model. Similarly, since prompts are very dependent on the model, when the model gets updated even through fine-tuning, your prompt may need to be replaced. In these situations having the model embedded in the application will contribute to simplify the versioning and traceability of the development cycle."
msgstr "*モデルとアプリケーションの類似したライフサイクル* ：モデルとそれを使用するアプリケーションのライフサイクルが同じで、アプリケーションの新機能の開発がモデルの変更を必要とするようなユースケースがありえます。同様に、プロンプトはモデルに大きく依存するため、微調整を経てでもモデルが更新されると、プロンプトを置き換える必要があるかもしれません。このような状況において、モデルをアプリケーションに埋め込むことは、開発サイクルのバージョニングとトレーサビリティの簡素化に貢献します。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Fast development/prototyping*: Not having to install, configure and interact with an external server can make the development of a LLM-based Java application much easier."
msgstr "*迅速な開発/プロトタイピング* ：外部サーバーをインストール、設定、操作する必要がないため、LLMベースのJavaアプリケーションの開発が非常に容易になります。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Easy models testing*: Running the LLM inference embedded in the JVM also makes it easier to test different models and their integration during the development phase."
msgstr "*モデルのテストが容易* JVMに組み込まれたLLM推論を実行することで、開発段階でのさまざまなモデルやその統合のテストも容易になります。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Security*: Performing the model inference in the same JVM instance that is run the application using it, eliminates the need of interacting with the LLM only through REST calls, thus preventing the leak of private data and allowing to enforce user authorization at a much finer grain."
msgstr "*セキュリティ* ：モデル推論を、それを使用してアプリケーションを実行するのと同じJVMインスタンスで実行することで、RESTコールを通じてのみLLMとやり取りする必要がなくなります。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Monolithic applications support*: The former point will be also beneficial for users still running monolithic applications, who in this way will be also able to include LLM-based capabilities in those applications without changing their architecture or platform."
msgstr "*モノリシック・アプリケーションのサポート* ：前者の点は、現在もモノリシックなアプリケーションを実行しているユーザーにとっても有益で、このようにすることで、アーキテクチャやプラットフォームを変更することなく、LLMベースの機能をアプリケーションに含めることができます。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Monitoring and Observability*: Running the LLM inference in pure Java will also allow simplify monitoring and observability, gathering statistics on the reliability and speed of the LLM response."
msgstr "*モニタリングと観測性* ：純粋なJavaでLLM推論を実行することで、LLM応答の信頼性と速度に関する統計を収集し、監視と観測を簡素化することもできます。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Developer Experience*: Debuggability will be simplified in the same way, allowing the Java developer to also navigate and debug the Jlama code if necessary."
msgstr "*開発者の体験* ：デバッグ性も同様に簡素化され、Java開発者も必要に応じてJlamaコードをナビゲートし、デバッグできるようになります。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Distribution*: Having the possibility to run LLM inference embedded in the same Java process will also make it possible to include the model itself into the same application package of the application using it (even though this could probably be advisable only in very specific circumstances)."
msgstr "*配布* ：LLM推論を同じJavaプロセスに組み込んで実行する可能性を持つことで、モデル自体を、それを使うアプリケーションの同じアプリケーション・パッケージに含めることも可能になります（これはおそらく、非常に特殊な状況でのみ推奨されるかもしれませんが）。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Edge friendliness*: The possibility of implementing and deploying a self-contained LLM-capable Java application will also make it a better fit than a client/server architecture for edge environments."
msgstr "*エッジでの使いやすさ* ：自己完結型のLLM対応Javaアプリケーションの実装とデプロイが可能なため、クライアント/サーバー・アーキテクチャよりもエッジ環境に適しています。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Embedding of auxiliary LLMs*: Many applications, especially the ones relying on agentic AI patterns, uses many different LLMs at once. For instance a smaller LLM could be used to validate and approve the responses of the main bigger one. In this case an hybrid approach could be convenient, embedding the smaller auxiliary LLMs while keeping serving the main one through a dedicated server."
msgstr "*補助LLMの埋め込み* ：多くのアプリケーション、特にエージェントAIパターンに依存するものは、一度に多くの異なるLLMを使用します。例えば、より小さなLLMを、より大きなLLMの応答の検証や承認に使うことができます。このような場合、小さな補助LLMを組み込みつつ、メインのLLMは専用サーバーで処理するハイブリッド・アプローチが便利です。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The site summarizer: a pure Java LLM-based application"
msgstr "サイト要約：純粋なJava LLMベースのアプリケーション"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "To demonstrate how Quarkus, Langchain4j and Jlama make straightforward to create a pure Java LLM infused application, where the LLM inference is directly embedded in the same JVM running the application I created a https://github.com/mariofusco/site-summarizer[simple project] that uses a LLM to automatically generate the summarization of a Wikipedia page or more in general of a blog post taken from any website."
msgstr "Quarkus、Langchain4jとJlamaが、LLM推論がアプリケーションを実行している同じJVMに直接組み込まれている、純粋なJava LLM注入アプリケーションを作成するのがいかに簡単であるかを示すために、私は、ウィキペディアのページの要約を自動的に生成するためにLLMを使用する link:https://github.com/mariofusco/site-summarizer[簡単なプロジェクトを] 作成しました。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "This video demonstrates how the application works."
msgstr "このビデオでは、アプリケーションがどのように動作するかを示しています。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Out-of-the-box this project uses a https://huggingface.co/tjake/Llama-3.2-1B-Instruct-JQ4[small Llama-3.2 model with 4-bit quantization]. When the application is compiled for the first time the model is automatically downloaded locally by Jlama from the Huggingface repository. However it is possible to replace this model and experiment with any other one by simply editing the https://github.com/mariofusco/site-summarizer/blob/main/src/main/resources/application.properties#L4[quarkus.langchain4j.jlama.chat-model.model-name property] in the application.properties file."
msgstr "このプロジェクトはすぐに link:https://huggingface.co/tjake/Llama-3.2-1B-Instruct-JQ4[4ビット量子化の小さなLlama-3.2モデルを] 使用します。アプリケーションが初めてコンパイルされるとき、モデルは自動的にHuggingfaceリポジトリからJlamaによってローカルにダウンロードされます。しかし、application.propertiesファイルの link:https://github.com/mariofusco/site-summarizer/blob/main/src/main/resources/application.properties#L4[quarkus.langchain4j.jlama.chat-model.model-nameプロパティを] 編集するだけで、このモデルを置き換えたり、他のモデルで実験したりすることができます。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "After having compiled and packaged the project with `mvn clean package`, the simplest way to use it is launching the jar passing as argument the URL of the web page containing the article that you want to summarize, something like:"
msgstr "`mvn clean package` でプロジェクトをコンパイルしてパッケージ化した後、それを使う最も簡単な方法は、要約したい記事を含むウェブページの URL を引数として渡して jar を起動することです："

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "that will generate an output like the following:"
msgstr "これは以下のような出力を生成します："

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Note that it is necessary to launch the JVM with a few additional arguments that enable the access to the Vector API which is still a Java preview feature, but it is internally used by Jlama to speed up the computation."
msgstr "VectorAPIはまだJavaのプレビュー機能ですが、Jlamaが計算を高速化するために内部的に使用しています。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "As better clarified by the readme of the project, there's a dedicated operating mode to process Wikipedia pages and also the possibility to expose this service through a REST endpoint."
msgstr "プロジェクトのreadmeでより明確にされているように、ウィキペディアのページを処理する専用のオペレーティング・モードがあり、RESTエンドポイントを通してこのサービスを公開する可能性もあります。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The internal implementation of this project is relatively straightforward: after having programmatically extracted the text to be summarized from the HTML page containing it, that text is sent to Jlama to be processed via a usual Langchain4j AiService."
msgstr "このプロジェクトの内部実装は比較的簡単です：それを含むHTMLページから要約されるテキストをプログラムで抽出した後、そのテキストは通常のLangchain4j AiServiceを介して処理されるためにJlamaに送られます。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "As anticipated, despite this implementation looks identical to any other LLM inference engine integrations, in this case there isn't any remote call to an external service, but the LLM inference is performed directly inside the same JVM running the application."
msgstr "予想通り、この実装は他のLLM推論エンジンの統合と同じように見えますが、この場合、外部サービスへのリモート呼び出しはなく、LLM推論はアプリケーションを実行している同じJVM内で直接実行されます。"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The combination of the 2 trends of the increasing spread of small and tailored models and the adoption of these models in the enterprise software development world will very likely promote the use of similar solutions in the near future."
msgstr "スモール＆テーラード・モデルの普及と、エンタープライズ・ソフトウェア開発の世界におけるこれらのモデルの採用という2つのトレンドが組み合わさることで、近い将来、同様のソリューションの利用が促進される可能性は非常に高いでしょう。"
